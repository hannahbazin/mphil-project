{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "949e6b4d",
   "metadata": {},
   "source": [
    "# Network analysis\n",
    "\n",
    "This generates the three inputs for network analysis and calls the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bdcd705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae118a4",
   "metadata": {},
   "source": [
    "## Create the combined PPI list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d805379b",
   "metadata": {},
   "source": [
    "Load and reformat the downloaded interactomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6c25670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wg/_lmfmp3j465cx8tv39t1vmnh0000gn/T/ipykernel_12974/1759536536.py:8: DtypeWarning: Columns (1,2,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  biogrid_PPI = pd.read_csv(\"../data/networks/BIOGRID-MV-Physical-4.4.245.tab3.txt\", sep=\"\\t\")\n"
     ]
    }
   ],
   "source": [
    "# ====== 1. STRING ======\n",
    "# Load reformatted STRING data (reformatting done in R using biomaRt)\n",
    "string_PPI = pd.read_csv(\"../data/networks/string_PPI.csv\", sep=\",\")\n",
    "\n",
    "\n",
    "# ====== 2. BioGRID ======\n",
    "# Load BioGRID data\n",
    "biogrid_PPI = pd.read_csv(\"../data/networks/BIOGRID-MV-Physical-4.4.245.tab3.txt\", sep=\"\\t\")\n",
    "\n",
    "# Filter to keep only human interactions\n",
    "biogrid_PPI = biogrid_PPI[\n",
    "    (biogrid_PPI[\"Organism ID Interactor A\"] == 9606) &\n",
    "    (biogrid_PPI[\"Organism ID Interactor B\"] == 9606)\n",
    "]\n",
    "\n",
    "# Filter and reformat\n",
    "biogrid_PPI = biogrid_PPI[[\"Official Symbol Interactor A\", \"Official Symbol Interactor B\"]]\n",
    "biogrid_PPI.columns = [\"GeneA\", \"GeneB\"]\n",
    "biogrid_PPI.to_csv(\"../data/networks/biogrid_PPI.csv\", index=False)\n",
    "\n",
    "\n",
    "# ====== 3. HINT ======\n",
    "# Load HINT data\n",
    "hint_PPI = pd.read_csv(\"../data/networks/HINT_HomoSapiens_binary_hq.txt\", sep=\"\\t\")\n",
    "\n",
    "# Filter and reformat\n",
    "hint_PPI = hint_PPI[[\"Gene_A\", \"Gene_B\"]]\n",
    "hint_PPI.columns = [\"GeneA\", \"GeneB\"]\n",
    "hint_PPI.to_csv(\"../data/networks/hint_PPI.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad0b7e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Dataset  Num_Interactions\n",
      "0  BIOGRID            318015\n",
      "1   STRING            207024\n",
      "2     HINT            163435\n"
     ]
    }
   ],
   "source": [
    "# Create summary table\n",
    "ppi_counts = pd.DataFrame({\n",
    "    \"Dataset\": [\"BIOGRID\", \"STRING\", \"HINT\"],\n",
    "    \"Num_Interactions\": [\n",
    "        len(biogrid_PPI),\n",
    "        len(string_PPI),\n",
    "        len(hint_PPI)\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Print the table\n",
    "print(ppi_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5986c8",
   "metadata": {},
   "source": [
    "Combine all PPI networks into one list without duplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4789859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined PPI, length: 688474\n",
      "After removing rows with empty entry, length: 678651\n",
      "After removing self-loops, length: 666715\n",
      "After removing duplicates, length: 666715\n",
      "After removing bidirectional duplicates, length: 345547\n",
      "Combined PPI network saved as 'combined_PPI.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Combine all PPI networks\n",
    "combined_ppi = pd.concat([biogrid_PPI, string_PPI, hint_PPI], ignore_index=True)\n",
    "print(\"Combined PPI, length:\", len(combined_ppi))\n",
    "\n",
    "# Remove empty rows\n",
    "combined_ppi = combined_ppi.dropna()\n",
    "print(\"After removing rows with empty entry, length:\", len(combined_ppi))\n",
    "\n",
    "# Remove self-loops\n",
    "combined_ppi = combined_ppi[combined_ppi[\"GeneA\"] != combined_ppi[\"GeneB\"]]\n",
    "print(\"After removing self-loops, length:\", len(combined_ppi))\n",
    "\n",
    "# Remove duplicates in both directions\n",
    "combined_ppi.drop_duplicates()\n",
    "print(\"After removing duplicates, length:\", len(combined_ppi))\n",
    "combined_ppi[\"sorted_pair\"] = combined_ppi.apply(lambda row: tuple(sorted([row[\"GeneA\"], row[\"GeneB\"]])), axis=1)\n",
    "combined_ppi = combined_ppi.drop_duplicates(subset=\"sorted_pair\")\n",
    "combined_ppi = combined_ppi.drop(columns=\"sorted_pair\")\n",
    "print(\"After removing bidirectional duplicates, length:\", len(combined_ppi))\n",
    "\n",
    "# Save combined PPI\n",
    "combined_ppi.to_csv(\"../data/networks/combined_PPI.csv\", index=False)\n",
    "print(\"Combined PPI network saved as 'combined_PPI.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc060cd",
   "metadata": {},
   "source": [
    "## Create the drug-gene interaction list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b072e92",
   "metadata": {},
   "source": [
    "### DrugBank\n",
    "From DrugBank, I take the files bonds.csv, polypeptides.csv, and bio_entities.csv and combine them to generate a drug-gene interaction list containing the drug bank ID and then the gene name of the protein it interacts with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "96cb676d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataframes...\n",
      "\n",
      "\n",
      "Filtering dataframes...\n",
      "\n",
      "\n",
      "Merging dataframes...\n",
      "\n",
      "Number of unique approved DrugBank drugs: 1863\n",
      "Total DPI rows for approved drugs: 3698\n",
      "\n",
      "Total drugâ€“gene pairs: 2879\n",
      "With drug names: 2879\n",
      "\n",
      "Final DrugBank DPI head:\n",
      "              Drug_Name Drug_Target\n",
      "0            Cetuximab        EGFR\n",
      "1         Dornase alfa      DNASE1\n",
      "2  Denileukin diftitox       IL2RA\n",
      "3  Denileukin diftitox       IL2RB\n",
      "4           Etanercept         TNF\n",
      "\n",
      "Number of unique drugs: 1438\n",
      "Number of unique target genes: 790\n",
      "\n",
      "Final DrugBank DPI dimensions: (2873, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============== Load data ==============\n",
    "\n",
    "print(\"\\nLoading dataframes...\\n\")\n",
    "\n",
    "bonds = pd.read_csv(\"../data/networks/milner_drugbank_postgresql/bonds.csv\", sep=\",\", header=None)\n",
    "bonds.columns = [\n",
    "    \"id\", \"type\", \"drug_id\", \"biodb_id\", \"pdb_id\", \"position\",\n",
    "    \"pharmacological_action\", \"antagonist\", \"agonist\", \"substrate\",\n",
    "    \"inhibitor\", \"inducer\", \"other_action\", \"inducer_strength\",\n",
    "    \"inhibitor_strength\", \"induction_clinically_sig\", \"inhibition_clinically_sig\"\n",
    "]\n",
    "\n",
    "polypeptides = pd.read_csv(\"../data/networks/milner_drugbank_postgresql/polypeptides.csv\", sep=\",\", header=None)\n",
    "polypeptides.columns = [\n",
    "    \"uniprot_id\", \"name\", \"uniprot_name\", \"gene_name\", \"organism_id\",\n",
    "    \"molecular_weight\", \"theoretical_pi\", \"general_function\", \"specific_function\",\n",
    "    \"signal_regions\", \"transmembrane_regions\", \"pdb_ids\", \"genbank_gene_id\",\n",
    "    \"genbank_protein_id\", \"genecard_id\", \"locus\", \"genatlas_id\", \"hgnc_id\",\n",
    "    \"meta_cyc_id\", \"ncbi_sequence_ids\", \"tissue_specificity\", \"cofactor\",\n",
    "    \"subunit\", \"cellular_location\", \"amino_acid_sequence\", \"gene_sequence\"\n",
    "]\n",
    "\n",
    "bio_entities = pd.read_csv(\"../data/networks/milner_drugbank_postgresql/bio_entities.csv\", sep=\",\", header=None)\n",
    "bio_entities.columns = [\n",
    "    \"biodb_id\", \"name\", \"kind\", \"organism\"\n",
    "]\n",
    "\n",
    "drugs = pd.read_csv(\"../data/networks/milner_drugbank_postgresql/drugs.csv\", sep=\",\", header=None)\n",
    "drugs.columns = [\n",
    "    \"id\", \"type\", \"drugbank_id\", \"name\", \"state\", \"description\",\n",
    "    \"simple_description\", \"clinical_description\", \"cas_number\",\n",
    "    \"protein_formula\", \"protein_weight\", \"investigational\", \"approved\",\n",
    "    \"vet_approved\", \"experimental\", \"nutraceutical\", \"illicit\", \"withdrawn\",\n",
    "    \"moldb_mono_mass\", \"moldb_inchi\", \"moldb_inchikey\", \"moldb_smiles\",\n",
    "    \"moldb_average_mass\", \"moldb_formula\", \"synthesis_patent_id\",\n",
    "    \"protein_weight_details\", \"biotech_kind\"\n",
    "]\n",
    "\n",
    "# Rename column for clarity\n",
    "drugs.rename(columns={\"id\": \"drug_id\", \"name\": \"drug_name\"}, inplace=True)\n",
    "\n",
    "# Print head of each DataFrame\n",
    "# print(\"Bonds DataFrame head:\")\n",
    "# print(bonds.head())\n",
    "# print(\"\\nPolypeptides DataFrame head:\")\n",
    "# print(polypeptides.head())\n",
    "# print(\"\\nBio Entities DataFrame head:\")\n",
    "# print(bio_entities.head())\n",
    "# print(\"\\nDrugs DataFrame head:\")\n",
    "# print(drugs.head())\n",
    "\n",
    "\n",
    "# ============== Filter dataframes ==============\n",
    "\n",
    "print(\"\\nFiltering dataframes...\\n\")\n",
    "\n",
    "# Filter for target bonds with pharmacological action\n",
    "target_bonds = bonds[\n",
    "    (bonds[\"type\"] == \"TargetBond\") &\n",
    "    (bonds[\"pharmacological_action\"] == \"yes\")\n",
    "].copy()\n",
    "target_bonds = target_bonds[[\"drug_id\", \"biodb_id\"]]\n",
    "\n",
    "# Filter for human proteins\n",
    "polypeptides = polypeptides[polypeptides[\"organism_id\"] == 154].copy()\n",
    "polypeptides = polypeptides[[\"name\", \"gene_name\"]]\n",
    "\n",
    "# Filter for human proteins\n",
    "bio_entities = bio_entities[\n",
    "    (bio_entities[\"kind\"] == \"protein\") &\n",
    "    (bio_entities[\"organism\"] == \"Humans\")\n",
    "].copy()\n",
    "bio_entities = bio_entities[[\"biodb_id\", \"name\"]]\n",
    "\n",
    "# Filter for drugs approved or in clinical trials, exclude non-human, illicit, nutraceutical, or withdrawn\n",
    "drugs = drugs[\n",
    "    (drugs[\"approved\"] == 1) &\n",
    "    (drugs[\"withdrawn\"] == 0) &\n",
    "    (drugs[\"illicit\"] == 0) &\n",
    "    (drugs[\"nutraceutical\"] == 0)\n",
    "].copy()\n",
    "\n",
    "# Filter for id, drug bank id, and name\n",
    "drugs = drugs[[\"drug_id\", \"drugbank_id\", \"drug_name\"]]\n",
    "\n",
    "# ============== Merge dataframes ==============\n",
    "\n",
    "print(\"\\nMerging dataframes...\\n\")\n",
    "\n",
    "# Merge drug ID's with protein names\n",
    "drug_protein = target_bonds.merge(bio_entities, on=\"biodb_id\", how=\"left\")\n",
    "\n",
    "# Merge with gene names\n",
    "drug_gene = drug_protein.merge(polypeptides, on=\"name\", how=\"left\")\n",
    "\n",
    "# Merge with drug bank IDs and drug names\n",
    "drug_gene = drug_gene.merge(drugs, on=\"drug_id\", how=\"inner\") # Keep only rows where drug_id exists in both dfs\n",
    "\n",
    "# Check number of unique approved drugs before final formatting\n",
    "approved_only = drug_gene[drug_gene[\"drugbank_id\"].notna()]\n",
    "print(f\"Number of unique approved DrugBank drugs: {approved_only['drug_name'].nunique()}\")\n",
    "print(f\"Total DPI rows for approved drugs: {len(approved_only)}\")\n",
    "\n",
    "# Drop missing target gene names\n",
    "drug_gene = drug_gene.dropna(subset=[\"gene_name\"])\n",
    "\n",
    "# Extract and clean final DataFrame\n",
    "drug_gene_final = drug_gene[[\"drug_name\", \"gene_name\"]]\n",
    "drug_gene_final.columns = [\"Drug_Name\", \"Drug_Target\"]\n",
    "\n",
    "# Verify number of rows with named drugs\n",
    "total_rows = len(drug_gene_final)\n",
    "missing_names = drug_gene_final[\"Drug_Name\"].isna().sum()\n",
    "named_rows = total_rows - missing_names\n",
    "print(\"\\nTotal drugâ€“gene pairs:\", total_rows)\n",
    "print(\"With drug names:\", named_rows)\n",
    "\n",
    "# Remove rows with NaN and duplicates\n",
    "drug_gene_final = drug_gene_final.dropna()\n",
    "drug_gene_final = drug_gene_final.drop_duplicates()\n",
    "\n",
    "# Print key info\n",
    "print(\"\\nFinal DrugBank DPI head:\\n\", drug_gene_final.head())\n",
    "print(\"\\nNumber of unique drugs:\", drug_gene_final[\"Drug_Name\"].nunique())\n",
    "print(\"Number of unique target genes:\", drug_gene_final[\"Drug_Target\"].nunique())\n",
    "print(\"\\nFinal DrugBank DPI dimensions:\", drug_gene_final.shape)\n",
    "\n",
    "# Save final DPI list\n",
    "drug_gene_final.to_csv(\"../data/networks/drugbank_DPI.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b23dd5",
   "metadata": {},
   "source": [
    "### ChEMBL\n",
    "From CheMBL, I downloaded the chembl_35_sqlite.tar.gz file from version 35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f3ffc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying ChEMBL DPI...\n",
      "\n",
      "DPI head before target mapping:\n",
      "    molregno target_chembl_id\n",
      "0    179188        CHEMBL235\n",
      "1     33632        CHEMBL259\n",
      "2     33415        CHEMBL259\n",
      "3     33415       CHEMBL3795\n",
      "4    219299       CHEMBL3974\n",
      "\n",
      "Number of unique drugs: 673271\n",
      "Number of unique targets: 3768\n",
      "\n",
      "DPI shape before target mapping: (1088205, 2)\n",
      "\n",
      "Mapping ChEMBL target IDs to UniProt...\n",
      "\n",
      "DPI head after target mapping:\n",
      "    molregno target_chembl_id target_uniprot_id\n",
      "0    179188        CHEMBL235            P37231\n",
      "1     33632        CHEMBL259            P32245\n",
      "2     33415        CHEMBL259            P32245\n",
      "3     33415       CHEMBL3795            Q01726\n",
      "4    219299       CHEMBL3974            P25116\n",
      "\n",
      "DPI shape after target mapping: (1088205, 2)\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# ============== Connect to ChEMBL database ==============\n",
    "\n",
    "conn = sqlite3.connect(\"../data/networks/chembl_35/chembl_35_sqlite/chembl_35.db\")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "# ============== Query high-confidence durg-protein interactions ==============\n",
    "\n",
    "print(\"\\nQuerying ChEMBL DPI...\\n\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT\n",
    "    activities.molregno,\n",
    "    target_dictionary.chembl_id AS target_chembl_id\n",
    "FROM\n",
    "    activities\n",
    "JOIN\n",
    "    assays ON activities.assay_id = assays.assay_id\n",
    "JOIN\n",
    "    target_dictionary ON assays.tid = target_dictionary.tid\n",
    "JOIN\n",
    "    target_components ON target_dictionary.tid = target_components.tid\n",
    "JOIN\n",
    "    component_sequences ON target_components.component_id = component_sequences.component_id\n",
    "WHERE\n",
    "    activities.standard_relation = '='\n",
    "    AND activities.standard_type IN ('IC50', 'Ki', 'Kd', 'EC50')\n",
    "    AND activities.standard_value IS NOT NULL\n",
    "    AND component_sequences.organism = 'Homo sapiens'\n",
    "\"\"\"\n",
    "\n",
    "chembl_dpi = pd.read_sql(query, conn)\n",
    "print(\"DPI head before target mapping:\\n\", chembl_dpi.head())\n",
    "print(\"\\nNumber of unique drugs:\", chembl_dpi[\"molregno\"].nunique())\n",
    "print(\"Number of unique targets:\", chembl_dpi[\"target_chembl_id\"].nunique())\n",
    "print(\"\\nDPI shape before target mapping:\", chembl_dpi.shape)\n",
    "\n",
    "# ============== Map target ChEMBL IDs to UniProt IDs ==============\n",
    "\n",
    "print(\"\\nMapping ChEMBL target IDs to UniProt...\\n\")\n",
    "\n",
    "uniprot_map = pd.read_csv(\"../data/networks/chembl_35/chembl_35_sqlite/chembl_uniprot_mapping.txt\", sep=\"\\t\", header=None, skiprows=1)\n",
    "uniprot_map.columns = [\"target_uniprot_id\", \"target_chembl_id\", \"target_name\", \"target_type\"]\n",
    "uniprot_map = uniprot_map[[\"target_chembl_id\", \"target_uniprot_id\"]]\n",
    "\n",
    "chembl_dpi_final = chembl_dpi.merge(uniprot_map, on=\"target_chembl_id\", how=\"left\")\n",
    "chembl_dpi_final = chembl_dpi_final.dropna()\n",
    "print(\"DPI head after target mapping:\\n\", chembl_dpi_final.head())\n",
    "print(\"\\nDPI shape after target mapping:\", chembl_dpi.shape)\n",
    "\n",
    "# Save intermediate DPI\n",
    "chembl_dpi_final.to_csv(\"../data/networks/inter_chembl_DPI.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3015aa",
   "metadata": {},
   "source": [
    "The mapping of target UniProt IDs to target gene names was done in R. The next step is to convert the drug molregno IDs to actual ChEMBL IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "47968445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading compound info and filtering...\n",
      "\n",
      "Summary of therapeutic stats after filtering:\n",
      "\n",
      "=== max_phase ===\n",
      "max_phase\n",
      "4.0    15357\n",
      "3.0      105\n",
      "2.0        2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== therapeutic_flag ===\n",
      "therapeutic_flag\n",
      "1    15464\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== molecule_type ===\n",
      "molecule_type\n",
      "Small molecule     15188\n",
      "Protein              234\n",
      "Unknown               39\n",
      "Oligosaccharide        2\n",
      "None                   1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final formatting and export...\n",
      "\n",
      "Before removing duplicates: 15464\n",
      "After removing duplicates: 14253\n",
      "Final ChEMBL DPI head:\n",
      "    Drug_Name Drug_Target\n",
      "0  CLONIDINE      ADRA1A\n",
      "1  CLONIDINE      ADRA1D\n",
      "2  CLONIDINE      ADRA1B\n",
      "3  CLONIDINE      ADRA2A\n",
      "4  CLONIDINE      ADRA2C\n",
      "\n",
      "Number of unique drugs: 1594\n",
      "Number of unique target genes: 1649\n",
      "\n",
      "Final ChEMBL DPI dimensions: (14253, 2)\n"
     ]
    }
   ],
   "source": [
    "# ============== Map target UniProt IDs to gene names in R ==============\n",
    "#\n",
    "# See network_reformatting_DPI.Rmd\n",
    "#\n",
    "# ============== Annotate with compound names and filter for therapeutic use ==============\n",
    "\n",
    "print(\"\\nLoading compound info and filtering...\\n\")\n",
    "\n",
    "# Load annotated DPI\n",
    "chembl_dpi = pd.read_csv(\"../data/networks/annotated_chembl_dpi.csv\", sep=\",\")\n",
    "\n",
    "# Query to get compound names\n",
    "compound_info_query = \"\"\"\n",
    "SELECT\n",
    "    molregno,\n",
    "    chembl_id AS compound_chembl_id,\n",
    "    pref_name AS compound_name\n",
    "FROM\n",
    "    molecule_dictionary\n",
    "\"\"\"\n",
    "compound_info = pd.read_sql(compound_info_query, conn)\n",
    "\n",
    "# Merge to get compound name\n",
    "chembl_dpi = chembl_dpi.merge(compound_info, on=\"molregno\", how=\"left\")\n",
    "\n",
    "# Query to get clinical phase and therapeutic use info\n",
    "compound_meta_query = \"\"\"\n",
    "SELECT chembl_id, max_phase, therapeutic_flag, molecule_type\n",
    "FROM molecule_dictionary\n",
    "\"\"\"\n",
    "compound_meta = pd.read_sql(compound_meta_query, conn)\n",
    "\n",
    "compound_meta = compound_meta[\n",
    "    (compound_meta[\"max_phase\"] >= 1) &        # Keep phase I or higher (approved or in clinical trials)\n",
    "    (compound_meta[\"therapeutic_flag\"] == 1)   # Keep compounds intended for therapeutic use\n",
    "]\n",
    "compound_meta = compound_meta.dropna(subset=[\"max_phase\"])\n",
    "compound_meta = compound_meta.dropna(subset=[\"therapeutic_flag\"])\n",
    "\n",
    "# Merge to retain only clinically relevant compounds\n",
    "chembl_dpi = chembl_dpi.merge(compound_meta, left_on=\"compound_chembl_id\", right_on=\"chembl_id\", how=\"inner\")\n",
    "chembl_dpi = chembl_dpi.drop(columns=[\"chembl_id\"])\n",
    "\n",
    "# Save intermediate DPI with compound names and meta info\n",
    "chembl_dpi_with_meta = chembl_dpi.copy()\n",
    "chembl_dpi_with_meta = chembl_dpi_with_meta.drop_duplicates(subset=[\"compound_name\", \"gene_name\"])\n",
    "\n",
    "# Verify filtering\n",
    "print(\"Summary of therapeutic stats after filtering:\")\n",
    "print(\"\\n=== max_phase ===\")\n",
    "print(chembl_dpi[\"max_phase\"].value_counts(dropna=False))\n",
    "print(\"\\n=== therapeutic_flag ===\")\n",
    "print(chembl_dpi[\"therapeutic_flag\"].value_counts(dropna=False))\n",
    "print(\"\\n=== molecule_type ===\")\n",
    "print(chembl_dpi[\"molecule_type\"].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "# ============== Final processing and export ==============\n",
    "\n",
    "print(\"\\nFinal formatting and export...\\n\")\n",
    "\n",
    "# Rename columns and remove unnamed drugs\n",
    "chembl_dpi_final = chembl_dpi[[\"compound_name\", \"gene_name\"]].copy()\n",
    "chembl_dpi_final.columns = [\"Drug_Name\", \"Drug_Target\"]\n",
    "chembl_dpi_final = chembl_dpi_final.dropna(subset=[\"Drug_Name\"])\n",
    "\n",
    "# Drop duplicates\n",
    "print(\"Before removing duplicates:\", len(chembl_dpi_final))\n",
    "chembl_dpi_final = chembl_dpi_final.drop_duplicates()\n",
    "print(\"After removing duplicates:\", len(chembl_dpi_final))\n",
    "\n",
    "# Print key info\n",
    "print(\"Final ChEMBL DPI head:\\n\", chembl_dpi_final.head())\n",
    "print(\"\\nNumber of unique drugs:\", chembl_dpi_final[\"Drug_Name\"].nunique())\n",
    "print(\"Number of unique target genes:\", chembl_dpi_final[\"Drug_Target\"].nunique())\n",
    "print(\"\\nFinal ChEMBL DPI dimensions:\", chembl_dpi_final.shape)\n",
    "\n",
    "# Save final ChEMBL DPI DataFrame\n",
    "chembl_dpi_final.to_csv(\"../data/networks/chembl_DPI.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2bcf306f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique approved drugs in ChEMBL DPI: 1588\n",
      "Total DPI rows for approved drugs: 14146\n"
     ]
    }
   ],
   "source": [
    "# Check how many unique approved drugs are in ChEMBL DPI\n",
    "approved_only = chembl_dpi_with_meta[chembl_dpi_with_meta[\"max_phase\"] == 4]\n",
    "print(f\"Number of unique approved drugs in ChEMBL DPI: {approved_only['compound_name'].nunique()}\")\n",
    "print(f\"Total DPI rows for approved drugs: {len(approved_only)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e57808",
   "metadata": {},
   "source": [
    "## Merge DrugBank and ChEMBL DPI lists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3f23002b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique drug names in ChEMBL: 1594\n",
      "Unique drug names in DrugBank: 1438\n",
      "Shared drug names: 783\n",
      "Combined DPI length before removing duplicates: 17126\n",
      "Combined DPI length after removing duplicates: 15951\n",
      "             Drug_Name Drug_Target\n",
      "0            cetuximab        EGFR\n",
      "1         dornase alfa      DNASE1\n",
      "2  denileukin diftitox       IL2RA\n",
      "3  denileukin diftitox       IL2RB\n",
      "4           etanercept         TNF\n",
      "Total unique drug-gene pairs: 15951\n",
      "Unique drugs: 2249\n",
      "Unique targets: 1933\n"
     ]
    }
   ],
   "source": [
    "# Load lists of drugâ€“gene pairs from DrugBank and ChEMBL\n",
    "drugbank_dpi = pd.read_csv(\"../data/networks/drugbank_DPI.csv\", sep=\",\")\n",
    "chembl_dpi = pd.read_csv(\"../data/networks/chembl_DPI.csv\", sep=\",\")\n",
    "\n",
    "# Standardise case and remove whitespace\n",
    "for df in [drugbank_dpi, chembl_dpi]:\n",
    "    df[\"Drug_Name\"] = df[\"Drug_Name\"].str.strip().str.lower()\n",
    "    df[\"Drug_Target\"] = df[\"Drug_Target\"].str.strip().str.upper()\n",
    "\n",
    "chembl_drugs = set(chembl_dpi[\"Drug_Name\"].unique())\n",
    "drugbank_drugs = set(drugbank_dpi[\"Drug_Name\"].unique())\n",
    "shared_drugs = chembl_drugs & drugbank_drugs\n",
    "print(f\"Unique drug names in ChEMBL: {len(chembl_drugs)}\")\n",
    "print(f\"Unique drug names in DrugBank: {len(drugbank_drugs)}\")\n",
    "print(f\"Shared drug names: {len(shared_drugs)}\")\n",
    "\n",
    "# Concatenate\n",
    "combined_dpi = pd.concat([drugbank_dpi, chembl_dpi], ignore_index=True)\n",
    "\n",
    "# Drop duplicates\n",
    "print(\"Combined DPI length before removing duplicates:\", len(combined_dpi))\n",
    "combined_dpi = combined_dpi.drop_duplicates()\n",
    "print(\"Combined DPI length after removing duplicates:\", len(combined_dpi))\n",
    "\n",
    "# Summary\n",
    "print(combined_dpi.head())\n",
    "print(f\"Total unique drug-gene pairs: {len(combined_dpi)}\")\n",
    "print(f\"Unique drugs: {combined_dpi['Drug_Name'].nunique()}\")\n",
    "print(f\"Unique targets: {combined_dpi['Drug_Target'].nunique()}\")\n",
    "\n",
    "# Save to CSV\n",
    "combined_dpi.to_csv(\"../data/networks/combined_DPI.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71872a03",
   "metadata": {},
   "source": [
    "## Create the disease gene lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "025e28b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      DEG\n",
      "0  ADAM10\n",
      "1    JAG1\n",
      "2    RHOA\n",
      "3     FN1\n",
      "4  FERMT2\n",
      "(70, 1)\n"
     ]
    }
   ],
   "source": [
    "# ====== STEP 1 ======\n",
    "\n",
    "# Load data\n",
    "step1 = pd.read_excel(\"../results/humanPVATsn/pathfindR/full/Sonia_network/step1_only_sonia.xls\")\n",
    "\n",
    "# Keep only gene columns\n",
    "step1_genes = step1[[\"Up_regulated_A\", \"Down_regulated_A\"]].copy()\n",
    "\n",
    "# Combine columns into one list\n",
    "step1_deg = (\n",
    "    step1_genes[\"Up_regulated_A\"].dropna().str.split(\", \").explode().tolist() +\n",
    "    step1_genes[\"Down_regulated_A\"].dropna().str.split(\", \").explode().tolist()\n",
    ")\n",
    "\n",
    "# Create the new DataFrame\n",
    "step1_deg = pd.DataFrame({\"DEG\": step1_deg})\n",
    "\n",
    "# Remove duplicates\n",
    "step1_deg = step1_deg.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(step1_deg.head())\n",
    "print(step1_deg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "92e075cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      DEG\n",
      "0     APP\n",
      "1  ZBTB20\n",
      "2  CAMK2D\n",
      "3   PRKG1\n",
      "4   DCLK1\n",
      "(99, 1)\n"
     ]
    }
   ],
   "source": [
    "# ====== STEP 2 ======\n",
    "\n",
    "# Load data\n",
    "step2 = pd.read_excel(\"../results/humanPVATsn/pathfindR/full/Sonia_network/step2_only_sonia.xls\")\n",
    "\n",
    "# Keep only gene columns\n",
    "step2_genes = step2[[\"Up_regulated_A\", \"Down_regulated_A\", \"Up_regulated_B\", \"Down_regulated_B\",]].copy()\n",
    "\n",
    "# Combine columns into one list\n",
    "step2_deg = (\n",
    "    step2_genes[\"Up_regulated_A\"].dropna().str.split(\", \").explode().tolist() +\n",
    "    step2_genes[\"Down_regulated_A\"].dropna().str.split(\", \").explode().tolist() +\n",
    "    step2_genes[\"Up_regulated_B\"].dropna().str.split(\", \").explode().tolist() +\n",
    "    step2_genes[\"Down_regulated_B\"].dropna().str.split(\", \").explode().tolist()\n",
    ")\n",
    "\n",
    "# Create the new DataFrame\n",
    "step2_deg = pd.DataFrame({\"DEG\": step2_deg})\n",
    "\n",
    "# Remove duplicates\n",
    "step2_deg = step2_deg.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(step2_deg.head())\n",
    "print(step2_deg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "6fc5eedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      DEG\n",
      "0   CDH11\n",
      "1  COL1A1\n",
      "2  COL1A2\n",
      "3    FBN1\n",
      "4   FGFR1\n",
      "(56, 1)\n"
     ]
    }
   ],
   "source": [
    "# ====== STEP 3 ======\n",
    "\n",
    "# Load data\n",
    "step3 = pd.read_excel(\"../results/humanPVATsn/pathfindR/full/Sonia_network/step3_only_sonia.xls\")\n",
    "\n",
    "# Keep only gene columns\n",
    "step3_genes = step3[[\"Up_regulated_B\", \"Down_regulated_B\",]].copy()\n",
    "\n",
    "# Combine columns into one list\n",
    "step3_deg = (\n",
    "    step3_genes[\"Up_regulated_B\"].dropna().str.split(\", \").explode().tolist() +\n",
    "    step3_genes[\"Down_regulated_B\"].dropna().str.split(\", \").explode().tolist()\n",
    ")\n",
    "\n",
    "# Create the new DataFrame\n",
    "step3_deg = pd.DataFrame({\"DEG\": step3_deg})\n",
    "\n",
    "# Remove duplicates\n",
    "step3_deg = step3_deg.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(step3_deg.head())\n",
    "print(step3_deg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "1b281f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       DEG\n",
      "0  PACSIN2\n",
      "1    ITGAV\n",
      "2    MYO9B\n",
      "3    ROBO1\n",
      "4    SLIT2\n",
      "(168, 1)\n"
     ]
    }
   ],
   "source": [
    "# ====== FULL DIFFERENTIATION ======\n",
    "\n",
    "# Load data\n",
    "full_diff = pd.read_excel(\"../results/humanPVATsn/pathfindR/full/Sonia_network/full_diff_only_sonia.xls\")\n",
    "\n",
    "# Keep only gene columns\n",
    "full_diff_genes = full_diff[[\"Up_regulated\", \"Down_regulated\",]].copy()\n",
    "\n",
    "# Combine columns into one list\n",
    "full_diff_deg = (\n",
    "    full_diff_genes[\"Up_regulated\"].dropna().str.split(\", \").explode().tolist() +\n",
    "    full_diff_genes[\"Down_regulated\"].dropna().str.split(\", \").explode().tolist()\n",
    ")\n",
    "\n",
    "# Create the new DataFrame\n",
    "full_diff_deg = pd.DataFrame({\"DEG\": full_diff_deg})\n",
    "\n",
    "# Remove duplicates\n",
    "full_diff_deg = full_diff_deg.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(full_diff_deg.head())\n",
    "print(full_diff_deg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "600847d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== SAVE FILES ======\n",
    "\n",
    "step1_deg.to_csv(\"../data/networks/step1_deg.csv\", index=False, header=False)\n",
    "step2_deg.to_csv(\"../data/networks/step2_deg.csv\", index=False, header=False)\n",
    "step3_deg.to_csv(\"../data/networks/step3_deg.csv\", index=False, header=False)\n",
    "full_diff_deg.to_csv(\"../data/networks/full_diff_deg.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4825d48",
   "metadata": {},
   "source": [
    "# Drug proximity analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32daf062",
   "metadata": {},
   "source": [
    "## Define functions\n",
    "The code from this chunk is taken from a paper by Han et al. (2021) titled \"Identification of SARS-CoV-2â€“induced pathways reveals drug repurposing strategies\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "58b3c079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def has_path(G,node_from,node_to): # returns true if the graph G has a path from source node(node_from) to target node (node_to)\n",
    "    return(nx.has_path(G,node_from,node_to))\n",
    "\n",
    "\n",
    "def get_shortest_path_length_between(G, source_id, target_id): #calculates shortest path length between source and target nodes in the graph G\n",
    "    return nx.shortest_path_length(G, source_id, target_id) \n",
    "\n",
    "'''\n",
    "The function below uses the previous two functions to calculate the path length \n",
    "between each source node (drug target) and its closest disease target.\n",
    "See Guney 2015 for more details on the approach (Network-based in silico drug efficacy screening).\n",
    "'''\n",
    "def calculate_closest_distance(network, nodes_from, nodes_to):\n",
    "    values_outer = []\n",
    "    for node_from in nodes_from: #nodes_from is a list of drug targets (eg. from drugbank, chembl, etc.)\n",
    "        values = [] # will store the shortest path length between a source node and all disease targets here\n",
    "        for node_to in nodes_to: #nodes_to is a list of known disease targets\n",
    "            # print(\"from - to\", node_from, node_to)\n",
    "            if not has_path(network,node_from,node_to): continue\n",
    "            val = get_shortest_path_length_between(network, node_from, node_to)\n",
    "            values.append(val)\n",
    "        if len(values) == 0:    continue\n",
    "        d = min(values) # the shortest path between a source node and its closest disease target\n",
    "        # print (d)\n",
    "        values_outer.append(d)\n",
    "    closest_d = numpy.mean(values_outer) # the average shortest path length between any source node (drug target) and its closest disease target\n",
    "    # print (d)\n",
    "    return closest_d\n",
    "\n",
    "\n",
    "def get_degree_binning(g, bin_size):\n",
    "    '''\n",
    "    This function creates the bins from a network g.\n",
    "    Starting from a list of nodes with the lowest degree, it adds nodes with the same degree to the bin until it reaches the set bin size.\n",
    "    If number of nodes with some degree is lower then bin size, it combines with other nodes with degree + 1 to meet bin size.\n",
    "    '''\n",
    "    degree_to_nodes = {}\n",
    "    # the below two lines compute the degree of each node in the graph.\n",
    "    # the setdefault method is used to add each node to a list of nodes with the same degree in the dictionary\n",
    "    for node, degree in g.degree(): \n",
    "        degree_to_nodes.setdefault(degree, []).append(node)\n",
    "    values = degree_to_nodes.keys()\n",
    "    values = sorted(values) # values becomes a list, sorted from lowest to highest degree\n",
    "    bins = []\n",
    "    i = 0 # this is the iterator that iterates over each degree, starting from the first item in the list (lowest degree)\n",
    "    while i < len(values):\n",
    "        low = values[i] # this is the i-th degree in the values list\n",
    "        val = degree_to_nodes[values[i]] # a list of the nodes with i-th degree (low)\n",
    "        while len(val) < bin_size:\n",
    "            # while the number of nodes in a bin is lower than the bin size, than nodes with degree i+1 will be added to the bin\n",
    "            # bin size is chosen by the user - in the paper this is set to 100\n",
    "            i += 1 # next iteration (move to the next degree in the list)\n",
    "            if i == len(values): # breaks when the last item in the list is reached\n",
    "                break\n",
    "            # starting from a list of nodes with the lowest degree, it adds nodes with degree lowest + 1 to the val list until it reaches the set bin size\n",
    "            val.extend(degree_to_nodes[values[i]]) # val will be extended with the next set of nodes with degree i+1 (low +1).\n",
    "        if i == len(values):\n",
    "            i -= 1\n",
    "        high = values[i] # this is the highest degree\n",
    "        i += 1\n",
    "        # print i, low, high, len(val)\n",
    "        if len(val) < bin_size:\n",
    "            low_, high_, val_ = bins[-1]\n",
    "            bins[-1] = (low_, high, val_ + val)\n",
    "        else:\n",
    "            bins.append((low, high, val))\n",
    "    return bins\n",
    "\n",
    "# this function lists all nodes in the same bin as each seed node\n",
    "def get_degree_equivalents(seeds, bins, g): \n",
    "    seed_to_nodes = {}\n",
    "    for seed in seeds:\n",
    "        d = g.degree(seed) #extract degree of the seed node\n",
    "        for l, h, nodes in bins: #it takes low, high degree and nodes for each bin\n",
    "            if l <= d and h >= d:\n",
    "                mod_nodes = list(nodes)\n",
    "                mod_nodes.remove(seed)\n",
    "                seed_to_nodes[seed] = mod_nodes\n",
    "                break\n",
    "    return seed_to_nodes\n",
    "\n",
    "    \n",
    "def pick_random_nodes_matching_selected(network, bins, nodes_selected, n_random, degree_aware=True, connected=False,\n",
    "                                        seed=None):\n",
    "    \"\"\"\n",
    "    Use get_degree_binning to get bins\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    values = []\n",
    "    nodes = network.nodes() # list of nodes in the network\n",
    "    for i in range(n_random): # decided by the user (how many times will the random iterations be repeated?) usually this is = 1000\n",
    "        if degree_aware:\n",
    "            if connected:\n",
    "                raise ValueError(\"Not implemented!\")\n",
    "            # the lines below pick random nodes matching the degree (same bin) of the real nodes\n",
    "            nodes_random = set()\n",
    "            node_to_equivalent_nodes = get_degree_equivalents(nodes_selected, bins, network) # lists nodes in the same bin as the node of interest\n",
    "            # now choose a random node from the same bin as the real node\n",
    "            for node, equivalent_nodes in node_to_equivalent_nodes.items():\n",
    "                chosen = random.choice(equivalent_nodes)\n",
    "                for k in range(20):  # Try to find a distinct node (at most 20 times) - to make sure it doesn't choose the same node\n",
    "                    if chosen in nodes_random:\n",
    "                        chosen = random.choice(equivalent_nodes)\n",
    "                nodes_random.add(chosen)\n",
    "            nodes_random = list(nodes_random)\n",
    "        else:\n",
    "            if connected:\n",
    "                nodes_random = [random.choice(nodes)]\n",
    "                k = 1\n",
    "                while True:\n",
    "                    if k == len(nodes_selected):\n",
    "                        break\n",
    "                    node_random = random.choice(nodes_random)\n",
    "                    node_selected = random.choice(network.neighbors(node_random))\n",
    "                    if node_selected in nodes_random:\n",
    "                        continue\n",
    "                    nodes_random.append(node_selected)\n",
    "                    k += 1\n",
    "            else:\n",
    "                nodes_random = random.sample(nodes, len(nodes_selected))\n",
    "        values.append(nodes_random)\n",
    "    return values\n",
    "\n",
    "def get_random_nodes(nodes, network, bins=None, n_random=1000, min_bin_size=100, degree_aware=True, seed=None):\n",
    "    '''\n",
    "    This function creates a n_random number of lists of random nodes with the same degree binning as the real nodes (when degree_aware=True).\n",
    "    usually n_random = 1000 because we often do 1000 iterations.\n",
    "    '''\n",
    "    if bins is None:\n",
    "        # Get degree bins of the network (if they aren't already supplied\n",
    "        bins = get_degree_binning(network, min_bin_size)\n",
    "    # pick the random nodes\n",
    "    nodes_random = pick_random_nodes_matching_selected(network, bins, nodes, n_random, degree_aware,\n",
    "                                                                         seed=seed)\n",
    "    return nodes_random\n",
    "\n",
    "def calculate_proximity(network, drug, nodes_from, nodes_to, nodes_from_random=None, nodes_to_random=None, bins=None,\n",
    "                        n_random=1000, min_bin_size=100, seed=452456):\n",
    "    \"\"\"\n",
    "    Calculate proximity from nodes_from to nodes_to\n",
    "    If degree binning or random nodes are not given, they are generated\n",
    "    \"\"\"\n",
    "\n",
    "    nodes_network = set(network.nodes())\n",
    "    nodes_from = set(nodes_from) & nodes_network # select only nodes_from (drug targets) that are located in the network\n",
    "    nodes_to = set(nodes_to) & nodes_network # select only nodes_to (disease targets) that are located in the network\n",
    "    if len(nodes_from) == 0 or len(nodes_to) == 0:\n",
    "        return None  # At least one of the node group not in network\n",
    "    d = calculate_closest_distance(network, nodes_from, nodes_to) # this is the real distance\n",
    "    \n",
    "    # now do 1000 iterations using random nodes\n",
    "    if bins is None and (nodes_from_random is None or nodes_to_random is None):\n",
    "        bins = get_degree_binning(network, min_bin_size)\n",
    "    if nodes_from_random is None:\n",
    "        nodes_from_random = get_random_nodes(nodes_from, network, bins=bins, n_random=n_random,\n",
    "                                             min_bin_size=min_bin_size, seed=seed)\n",
    "    if nodes_to_random is None:\n",
    "        nodes_to_random = get_random_nodes(nodes_to, network, bins=bins, n_random=n_random, min_bin_size=min_bin_size,\n",
    "                                           seed=seed)\n",
    "    random_values_list = zip(nodes_from_random, nodes_to_random)\n",
    "    values = numpy.empty(len(nodes_from_random))  # n_random\n",
    "    # now calculates the closest distance using random nodes. Repeat x1000\n",
    "    for i, values_random in enumerate(random_values_list):\n",
    "        #print('iteration ', i)\n",
    "        nodes_from, nodes_to = values_random\n",
    "        values[i] = calculate_closest_distance(network, nodes_from, nodes_to)\n",
    "    m, s = numpy.mean(values), numpy.std(values) # do mean and stdev of random iterations\n",
    "    if s == 0:\n",
    "        z = 0.0\n",
    "    else:\n",
    "        z = (d - m) / s\n",
    "    dict = {'drug': drug, 'distance': d, 'z_score': z}\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b87df3",
   "metadata": {},
   "source": [
    "## Call function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1676d70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import sys, time\n",
    "import numpy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e09e90c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in PPI graph: 18451\n",
      "Number of edges in PPI graph: 345547\n"
     ]
    }
   ],
   "source": [
    "# Create PPI network\n",
    "ppi_df = pd.read_csv(\"../data/networks/combined_PPI.csv\", sep=\",\")\n",
    "ppi_graph = nx.from_pandas_edgelist(ppi_df, \"GeneA\", \"GeneB\")\n",
    "\n",
    "# Get degree bins\n",
    "bins = get_degree_binning(ppi_graph, bin_size=100)\n",
    "\n",
    "# Print basic information about the graph\n",
    "print(f\"Number of nodes in PPI graph: {ppi_graph.number_of_nodes()}\")\n",
    "print(f\"Number of edges in PPI graph: {ppi_graph.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c35a614d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of drug targets retained in DPI: 3336\n"
     ]
    }
   ],
   "source": [
    "# Prepare DPI data\n",
    "dpi_df = pd.read_csv(\"../data/networks/combined_DPI.csv\", sep=\",\")\n",
    "\n",
    "# Drop NA\n",
    "dpi_df.dropna(inplace=True)\n",
    "\n",
    "# Filter DPI to keep only drug targets that are in the PPI graph\n",
    "dpi_df = dpi_df[dpi_df[\"Drug_Target\"].isin(ppi_graph.nodes)]\n",
    "drug_list = dpi_df[\"Drug_Name\"].unique()\n",
    "\n",
    "# Print basic information about the DPI\n",
    "print(f\"Number of drug targets retained in DPI: {len(drug_list)}\")\n",
    "\n",
    "# Create a mapping of drug names to their targets\n",
    "drug_to_targets = dpi_df.groupby(\"Drug_Name\")[\"Drug_Target\"].apply(set).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e5aa7b",
   "metadata": {},
   "source": [
    "### Define function for proximity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c96e4e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_proximity_analysis(step_name, deg_file_path, output_csv_path, ppi_graph, drug_to_targets,\n",
    "                           bins, n_random=1000, min_bin_size=100, seed=452456):\n",
    "    \"\"\"\n",
    "    Calculate proximity for step_name using drugs and targets in drug_to_targets and\n",
    "    disease genes in deg_file_path\n",
    "    Saves the result as a CSV file at output_csv_path\n",
    "    \"\"\"\n",
    "    print(f\"\\n===== Processing {step_name} =====\")\n",
    "    \n",
    "    # ========== LOAD DISEASE GENES ==========\n",
    "    disease_genes = pd.read_csv(deg_file_path, header=None)\n",
    "    print(len(disease_genes), f\"disease genes in {step_name}\")\n",
    "    disease_genes = disease_genes.iloc[:,0].dropna().unique().tolist()\n",
    "    # Keep only disease genes that are in the PPI graph\n",
    "    disease_genes = [gene for gene in disease_genes if gene in ppi_graph.nodes]\n",
    "    print(len(disease_genes), f\"disease genes in {step_name} after filtering for PPI graph\")\n",
    "    \n",
    "    # ========== GENERATE RANDOM SETS OF DISEASE GENES ==========\n",
    "    # For each disease gene, pick a random node from the same degree bin, and repeat this 1000 times,\n",
    "    # generating 1000 sets of random disease genes\n",
    "    random_disease_genes = get_random_nodes(\n",
    "        disease_genes,\n",
    "        ppi_graph,\n",
    "        bins=bins,\n",
    "        n_random=n_random,\n",
    "        min_bin_size=min_bin_size,\n",
    "        seed=seed,\n",
    "        degree_aware=True)\n",
    "    print(\"Generated\", len(random_disease_genes), \"sets of random disease genes\")\n",
    "\n",
    "    # ========== RUN PROXIMITY ANALYSIS ==========\n",
    "    results = []\n",
    "    # Iterate over each drug and calculate proximity to disease genes\n",
    "    for i, (drug, targets) in enumerate(drug_to_targets.items()):\n",
    "        start = time.time()\n",
    "        print(f\"Processing drug: {drug}\")\n",
    "\n",
    "        # Calculate proximity to disease genes\n",
    "        result = calculate_proximity(\n",
    "            ppi_graph,\n",
    "            drug,\n",
    "            targets,\n",
    "            disease_genes,\n",
    "            nodes_from_random=random_disease_genes,\n",
    "            bins=bins,\n",
    "            n_random=n_random,\n",
    "            min_bin_size=min_bin_size,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "        if result is not None:\n",
    "            results.append(result)\n",
    "        else:\n",
    "            print(f\"No proximity data for drug: {drug}\")\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f\"{i}/{len(drug_to_targets)} drugs processed\")\n",
    "\n",
    "        end = time.time()\n",
    "        print(f\"Runtime: {end - start:.2f} sec\")\n",
    "\n",
    "    # Save and display\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Saved results to {output_csv_path}\")\n",
    "    print(\"Top 10 prioritised drugs for\", step_name, results_df.sort_values(\"z_score\").head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f7ca5e",
   "metadata": {},
   "source": [
    "### Call function for each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc73032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DEG file paths and output paths\n",
    "steps = {\n",
    "    \"step1\": {\n",
    "        \"deg_file\": \"../data/networks/step1_deg.csv\",\n",
    "        \"output_csv\": \"../results/humanPVATsn/network_analysis/proximity_step1.csv\"\n",
    "    },\n",
    "    \"step2\": {\n",
    "        \"deg_file\": \"../data/networks/step2_deg.csv\",\n",
    "        \"output_csv\": \"../results/humanPVATsn/network_analysis/proximity_step2.csv\"\n",
    "    },\n",
    "    \"step3\": {\n",
    "        \"deg_file\": \"../data/networks/step3_deg.csv\",\n",
    "        \"output_csv\": \"../results/humanPVATsn/network_analysis/proximity_step3.csv\"\n",
    "    },\n",
    "    \"full\": {\n",
    "        \"deg_file\": \"../data/networks/full_diff_deg.csv\",\n",
    "        \"output_csv\": \"../results/humanPVATsn/network_analysis/proximity_full.csv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run proximity analysis for each step\n",
    "for step, path in steps.items():\n",
    "    run_proximity_analysis(step_name=step,\n",
    "                           deg_file_path=path[\"deg_file\"],\n",
    "                           output_csv_path=path[\"output_csv\"],\n",
    "                           ppi_graph=ppi_graph,\n",
    "                           drug_to_targets=drug_to_targets,\n",
    "                           bins=bins,\n",
    "                           n_random=1000,\n",
    "                           min_bin_size=100,\n",
    "                           seed=452456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe9aa5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
